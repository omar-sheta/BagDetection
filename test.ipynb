{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and predict\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2 as cv\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Users/Omar/Desktop/robocup/bag detection/runs/detect/train4\u001b[0m\n",
      "├── \u001b[00margs.yaml\u001b[0m\n",
      "├── \u001b[00mevents.out.tfevents.1703882106.Omars-MBP-6.11394.0\u001b[0m\n",
      "├── \u001b[00mresults.csv\u001b[0m\n",
      "├── \u001b[01;35mtrain_batch0.jpg\u001b[0m\n",
      "├── \u001b[01;35mtrain_batch1.jpg\u001b[0m\n",
      "├── \u001b[01;35mtrain_batch2.jpg\u001b[0m\n",
      "└── \u001b[01;34mweights\u001b[0m\n",
      "    ├── \u001b[00mbest.pt\u001b[0m\n",
      "    └── \u001b[00mlast.pt\u001b[0m\n",
      "\n",
      "2 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "# tree this /Users/Omar/Desktop/robocup/bag detection/runs/detect/train4\n",
    "!tree /Users/Omar/Desktop/robocup/bag\\ detection/runs/detect/train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the trained model file\n",
    "model_path = \"/Users/Omar/Desktop/robocup/bag detection/runs/detect/train4/weights/best.pt\"\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "# img_path = '/robocup/bag detection/test.png'\n",
    "# img_path = '/robocup/test1.jpeg'\n",
    "img_path = '/Users/Omar/Desktop/robocup/bag detection/find-luggage-1/valid/images/bag_060_jpg.rf.33639af8da68b952a3586e43288ef37d.jpg'\n",
    "img = cv.imread(img_path)\n",
    "# img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "img = cv.resize(img, (1024, 704))  # Resize as per the model's input size\n",
    "img = img.transpose(2, 0, 1)  # Change channel order\n",
    "img = img[None, :, :, :]  # Add batch dimension\n",
    "img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "img_tensor = torch.from_numpy(img)  # Convert to torch tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x1024 1 paper bag, 4592.1ms\n",
      "Speed: 0.1ms preprocess, 4592.1ms inference, 66.1ms postprocess per image at shape (1, 3, 704, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'paper bag'}\n",
      "orig_img: array([[[239, 239, 239],\n",
      "        [239, 239, 239],\n",
      "        [240, 240, 240],\n",
      "        ...,\n",
      "        [239, 239, 239],\n",
      "        [238, 238, 238],\n",
      "        [238, 238, 238]],\n",
      "\n",
      "       [[241, 241, 241],\n",
      "        [241, 241, 241],\n",
      "        [242, 242, 242],\n",
      "        ...,\n",
      "        [241, 241, 241],\n",
      "        [240, 240, 240],\n",
      "        [240, 240, 240]],\n",
      "\n",
      "       [[242, 242, 242],\n",
      "        [243, 243, 243],\n",
      "        [243, 243, 243],\n",
      "        ...,\n",
      "        [242, 242, 242],\n",
      "        [242, 242, 242],\n",
      "        [241, 241, 241]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[241, 241, 241],\n",
      "        [241, 241, 241],\n",
      "        [242, 242, 242],\n",
      "        ...,\n",
      "        [242, 242, 242],\n",
      "        [241, 241, 241],\n",
      "        [241, 241, 241]],\n",
      "\n",
      "       [[240, 240, 240],\n",
      "        [240, 240, 240],\n",
      "        [241, 241, 241],\n",
      "        ...,\n",
      "        [241, 241, 241],\n",
      "        [240, 240, 240],\n",
      "        [240, 240, 240]],\n",
      "\n",
      "       [[238, 238, 238],\n",
      "        [238, 238, 238],\n",
      "        [239, 239, 239],\n",
      "        ...,\n",
      "        [239, 239, 239],\n",
      "        [238, 238, 238],\n",
      "        [238, 238, 238]]], dtype=uint8)\n",
      "orig_shape: (704, 1024)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 0.11301040649414062, 'inference': 4592.097043991089, 'postprocess': 66.09702110290527}]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(img_tensor, conf=0.5)  # Get the results\n",
    "\n",
    "print(result)  # Print the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     218.64      9.3561      821.04      659.08]] paper bag\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = result[0].boxes.xyxy.numpy()  # Get bounding boxes\n",
    "name = result[0].names[0]  # Get the class name\n",
    "print(bounding_boxes, name)  # Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the original image again for drawing\n",
    "orig_img = cv.imread(img_path)\n",
    "# orig_img = cv.cvtColor(orig_img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "# No resizing needed for the original image\n",
    "# print width and height of the image\n",
    "print(orig_img.shape)\n",
    "\n",
    "\n",
    "for box in bounding_boxes:\n",
    "    # Coordinates - Convert to integers\n",
    "    x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv.rectangle(orig_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Put label with class name\n",
    "    label = f'{name}'\n",
    "    cv.putText(orig_img, label, (x1, y1 + 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv.imshow('Detected Image', orig_img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9325])\n",
      "data: tensor([[  8.2898,   1.9543, 416.0000, 416.0000,   0.9325,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (416, 416)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[212.1449, 208.9772, 407.7102, 414.0457]])\n",
      "xywhn: tensor([[0.5100, 0.5023, 0.9801, 0.9953]])\n",
      "xyxy: tensor([[  8.2898,   1.9543, 416.0000, 416.0000]])\n",
      "xyxyn: tensor([[0.0199, 0.0047, 1.0000, 1.0000]])\n",
      "{0: 'paper bag'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
